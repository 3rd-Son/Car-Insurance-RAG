{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/appleplay/miniconda3/envs/caserader/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/Users/appleplay/miniconda3/envs/caserader/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "from constants import EMBEDDINGS, index, CHAT_LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and see the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"bestdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below I will chunk the data in such a way that each chunk represents a query-response pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the chunks\n",
    "chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 5024.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the rows of the DataFrame\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    query = row['Query']\n",
    "    response = row['Response']\n",
    "    \n",
    "    # Treat each query-response pair as a separate chunk\n",
    "    chunk = [f\"question: {query}\\n answer: {response}\"]\n",
    "    chunks.append(chunk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below I am embedding the chunked dataset using OpenAI's text-embedding-ada-00 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "def generate_embeddings(documents: list[any]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list[any]): A list of document objects, each containing a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        list[list[float]]: A list containig a list of embeddings corresponding to the documents.\n",
    "    \"\"\"\n",
    "    embedded = [EMBEDDINGS.embed_documents(doc) for doc in documents]\n",
    "    return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "chunked_document_embeddings = generate_embeddings(documents=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_document_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below I am creating a dictionary that will be a combination of the chunked text, the embeddings and a unique id for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_short_id(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a short ID based on the content using SHA-256 hash.\n",
    "\n",
    "    Args:\n",
    "    - content (str): The content for which the ID is generated.\n",
    "\n",
    "    Returns:\n",
    "    - short_id (str): The generated short ID.\n",
    "    \"\"\"\n",
    "    hash_obj = hashlib.sha256()\n",
    "    hash_obj.update(content.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()\n",
    "\n",
    "\n",
    "def combine_vector_and_text(\n",
    "    documents: list[any], doc_embeddings: list[list[float]]\n",
    ") -> list[dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Process a list of documents along with their embeddings.\n",
    "\n",
    "    Args:\n",
    "    - documents (List[Any]): A list of documents (strings or other types).\n",
    "    - doc_embeddings (List[List[float]]): A list of embeddings corresponding to the documents.\n",
    "\n",
    "    Returns:\n",
    "    - data_with_metadata (List[Dict[str, Any]]): A list of dictionaries, each containing an ID, embedding values, and metadata.\n",
    "    \"\"\"\n",
    "    data_with_metadata = []\n",
    "\n",
    "    for doc_text, embedding in zip(documents, doc_embeddings):\n",
    "        # Convert doc_text to string if it's not already a string\n",
    "        if not isinstance(doc_text, str):\n",
    "            doc_text = str(doc_text)\n",
    "\n",
    "        # Generate a unique ID based on the text content\n",
    "        doc_id = generate_short_id(doc_text)\n",
    "\n",
    "        # Create a data item dictionary\n",
    "        data_item = {\n",
    "            \"id\": doc_id,\n",
    "            \"values\": embedding[0],\n",
    "            \"metadata\": {\"text\": doc_text},  # Include the text as metadata\n",
    "        }\n",
    "\n",
    "        # Append the data item to the list\n",
    "        data_with_metadata.append(data_item)\n",
    "\n",
    "    return data_with_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "data_with_meta_data = combine_vector_and_text(documents=chunks, doc_embeddings=chunked_document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_with_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pinecone.data.index.Index object at 0x115039e10>\n"
     ]
    }
   ],
   "source": [
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I am sending the data to my pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_data_to_pinecone(data_with_metadata: list[dict[str, any]]) -> None:\n",
    "    \"\"\"\n",
    "    Upsert data with metadata into a Pinecone index.\n",
    "\n",
    "    Args:\n",
    "    - data_with_metadata (List[Dict[str, Any]]): A list of dictionaries, each containing data with metadata.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    index.upsert(vectors=data_with_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "upsert_data_to_pinecone(data_with_metadata= data_with_meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embeddings(query: str) -> list[float]:\n",
    "    \"\"\"This function returns a list of the embeddings for a given query\n",
    "\n",
    "    Args:\n",
    "        query (str): The actual query/question\n",
    "\n",
    "    Returns:\n",
    "        list[float]: The embeddings for the given query\n",
    "    \"\"\"\n",
    "    query_embeddings = EMBEDDINGS.embed_query(query)\n",
    "    return query_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the pipeline by querying the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "user_question = \"Will I get a refund if I cancel my policy after making a claim?\"\n",
    "query_embeddings = get_query_embeddings(query=user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the vector database to retrieve the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone_index(\n",
    "    query_embeddings: list, top_k: int = 2, include_metadata: bool = True\n",
    ") -> dict[str, any]:\n",
    "    \"\"\"\n",
    "    Query a Pinecone index.\n",
    "\n",
    "    Args:\n",
    "    - index (Any): The Pinecone index object to query.\n",
    "    - vectors (List[List[float]]): List of query vectors.\n",
    "    - top_k (int): Number of nearest neighbors to retrieve (default: 2).\n",
    "    - include_metadata (bool): Whether to include metadata in the query response (default: True).\n",
    "\n",
    "    Returns:\n",
    "    - query_response (Dict[str, Any]): Query response containing nearest neighbors.\n",
    "    \"\"\"\n",
    "    query_response = index.query(\n",
    "        vector=query_embeddings, top_k=top_k, include_metadata=include_metadata\n",
    "    )\n",
    "    return query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "answers = query_pinecone_index(query_embeddings=query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'abee985c5e9c2b6136d1ffd966e2d3451b8bbe8557c327ceb4bbce0407364a20',\n",
       "              'metadata': {'text': \"['question: Will I get a refund if I \"\n",
       "                                   'cancel my policy after making a claim?\\\\n '\n",
       "                                   'answer: No, if you’ve made a claim, the '\n",
       "                                   'insurer will not refund any car insurance '\n",
       "                                   \"premium.']\"},\n",
       "              'score': 0.911601782,\n",
       "              'values': []},\n",
       "             {'id': 'f29470e3938049bca53eb4a1ef84102802f6cce7631859be1106194abeed0de0',\n",
       "              'metadata': {'text': \"['question: Will the insurer give me proof \"\n",
       "                                   'of my No Claim Discount if I cancel the '\n",
       "                                   'policy?\\\\n answer: Yes, the insurer will '\n",
       "                                   'give you proof of any No Claim Discount if '\n",
       "                                   \"you cancel the policy.']\"},\n",
       "              'score': 0.861411,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the text from the dictionary before passing it to the LLM\n",
    "text_answer = \" \".join([doc['metadata']['text'] for doc in answers['matches']])\n",
    "\n",
    "# prompt = f\"{text_answer} Using the provided information, give me a better and summarized answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an experienced insurance professional with deep knowledge of car insurance policies. Your task is to provide accurate and concise responses to queries based on a given car insurance policy document.\n",
    "You will receive two inputs:\n",
    "1. The user's question related to the car insurance policy.\n",
    "2. The answer gotten from the database.\n",
    "Your role is to summarize the retrieved information and craft a clear, well-structured response that directly answers the user's question. \n",
    "# Keep your responses straightforward and easy to understand forvfvvvvfxbv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_prompt = f\"{SYSTEM_PROMPT}\\n\\This is the question: {user_question}\\nThis is the answer from the database: {text_answer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an experienced insurance professional with deep knowledge of car insurance policies. Your task is to provide accurate and concise responses to queries based on a given car insurance policy document.\\nYou will receive two inputs:\\n1. The original query related to the car insurance policy.\\n2. The relevant excerpt retrieved from the policy document based on the query.\\nYour role is to analyze the retrieved information and craft a clear, well-structured response that directly answers the query. Draw upon your expertise in insurance policies to provide additional context or clarification if needed.\\nIf the retrieved information is insufficient to fully answer the query, summarize what you can based on the excerpt, and indicate that the available information is limited. \\nKeep your responses straightforward and easy to understand for general audiences. Define any technical terms if necessary, and maintain a professional, informative tone befitting an experienced insurance professional.\\n\\n\\\\This is the question: Will I get a refund if I cancel my policy after making a claim?\\nThis is the answer from the database: ['question: Will I get a refund if I cancel my policy after making a claim?\\\\n answer: No, if you’ve made a claim, the insurer will not refund any car insurance premium.'] ['question: Will the insurer give me proof of my No Claim Discount if I cancel the policy?\\\\n answer: Yes, the insurer will give you proof of any No Claim Discount if you cancel the policy.']\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_query_response(prompt: str) -> str:\n",
    "    \"\"\"This function returns a better response using LLM\n",
    "    Args:\n",
    "        prompt (str): The prompt template\n",
    "\n",
    "    Returns:\n",
    "        str: The actual response returned by the LLM\n",
    "    \"\"\"\n",
    "    better_answer = CHAT_LLM(prompt)\n",
    "    return better_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "final_answer = better_query_response(prompt=LLM_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Based on the provided policy document, if you cancel your policy after making a claim, you will not receive a refund for any car insurance premiums. This means that the insurer will not reimburse you for any unused portion of your policy. However, if you cancel your policy without making a claim, the insurer will provide you with proof of your No Claim Discount. This is a discount given to drivers who have not made any claims during their policy term. Please note that if you have made a claim, you may not be eligible for this discount. If you have any further questions or concerns regarding your policy, please do not hesitate to contact your insurer for more information.\n"
     ]
    }
   ],
   "source": [
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
